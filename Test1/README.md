# Batch Normalization & Activation
我反复检查了自己的数据构建，并没有发现什么问题。断点调试发现有些地方特征图数值非常小，有的甚至成了负数，检查后发现很多卷积块、attention块后面没加归一化和激活函数（汗、、）。查阅了一下BN和LN的区别后，我在卷积块后面补充了BN，在Transformer块后面补充了LN。
但是最终效果仍在18%左右。


## 批归一化通过对小批量数据的每个特征进行归一化处理，使输出值近似为均值为0、标准差为1的分布。
具体来说，对于每个特征，BN计算该批次数据中的均值和方差，并用它们来归一化数据。然后通过缩放（scale）和位移（shift）参数进一步调整，这两个参数是在网络训练过程中学习得到的。
### 使用范围：主要用于全连接层和卷积层。
在训练中效果最佳，因为它依赖于每个批次的数据分布。
适合批次较大且稳定的场景，不适合批次大小为1或较小的情况。
### 优点：
可以使用更高的学习率，加速网络训练。
对初始化不那么敏感。
### 缺点：
对批次大小比较敏感，小批次时效果不稳定。
在推理（inference）时需要使用整个训练集的统计数据，增加了计算复杂性。
## 层归一化对单个样本的所有激活进行归一化，而不是批归一化中对批内单一特征进行归一化。
对每个样本，LN计算所有输入的均值和方差，然后使用这些统计数据来归一化输入层。
## 使用范围：
适用于全连接层、卷积层和递归神经网络层。
特别适用于批次大小为1或比较小的情况，如在线学习或强化学习中的应用。
可以稳定地用在RNNs和Transformers等模型中。
## 优点：
不依赖于批次的大小，因此在批次大小变化或很小的情况下依然表现稳定。
简化了模型的推理步骤，因为归一化不依赖于整个数据集的统计数据。
## 缺点：
在某些任务中可能不如批归一化有效，特别是当批次较大时。
